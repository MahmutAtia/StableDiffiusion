{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNAjLLAj5DO/GIVJfyViM4W"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["! wget https://raw.githubusercontent.com/openimages/dataset/master/downloader.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dX1rQbPdYqaP","executionInfo":{"status":"ok","timestamp":1675329098418,"user_tz":-180,"elapsed":363,"user":{"displayName":"Mahmoud Atia Ead Atia","userId":"11781797524477583668"}},"outputId":"ec585c01-74b0-4abe-c73e-a4793af1774e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-02-02 09:11:37--  https://raw.githubusercontent.com/openimages/dataset/master/downloader.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4244 (4.1K) [text/plain]\n","Saving to: ‘downloader.py’\n","\n","\rdownloader.py         0%[                    ]       0  --.-KB/s               \rdownloader.py       100%[===================>]   4.14K  --.-KB/s    in 0s      \n","\n","2023-02-02 09:11:37 (58.1 MB/s) - ‘downloader.py’ saved [4244/4244]\n","\n"]}]},{"cell_type":"code","source":["! wget https://raw.githubusercontent.com/sangyun884/HR-VITON/main/networks.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zkhwKFanaC_A","executionInfo":{"status":"ok","timestamp":1675330974734,"user_tz":-180,"elapsed":328,"user":{"displayName":"Mahmoud Atia Ead Atia","userId":"11781797524477583668"}},"outputId":"4468849b-0a11-4d2e-8f0d-4a207ad9bcaf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-02-02 09:42:54--  https://raw.githubusercontent.com/sangyun884/HR-VITON/main/networks.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 19182 (19K) [text/plain]\n","Saving to: ‘networks.py’\n","\n","\rnetworks.py           0%[                    ]       0  --.-KB/s               \rnetworks.py         100%[===================>]  18.73K  --.-KB/s    in 0s      \n","\n","2023-02-02 09:42:54 (114 MB/s) - ‘networks.py’ saved [19182/19182]\n","\n"]}]},{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/sangyun884/HR-VITON/main/cp_dataset_test.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l-9mwneUmKE_","executionInfo":{"status":"ok","timestamp":1675332468477,"user_tz":-180,"elapsed":289,"user":{"displayName":"Mahmoud Atia Ead Atia","userId":"11781797524477583668"}},"outputId":"9b15e107-64d4-41a7-ff00-a695f0018e45"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-02-02 10:07:47--  https://raw.githubusercontent.com/sangyun884/HR-VITON/main/cp_dataset_test.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 11703 (11K) [text/plain]\n","Saving to: ‘cp_dataset_test.py’\n","\n","\rcp_dataset_test.py    0%[                    ]       0  --.-KB/s               \rcp_dataset_test.py  100%[===================>]  11.43K  --.-KB/s    in 0s      \n","\n","2023-02-02 10:07:47 (113 MB/s) - ‘cp_dataset_test.py’ saved [11703/11703]\n","\n"]}]},{"cell_type":"code","source":["from networks import ConditionGenerator"],"metadata":{"id":"ladTudjHgfEs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget https://www.dropbox.com/s/10bfat0kg4si1bu/zalando-hd-resized.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CPIf-xT4iJde","executionInfo":{"status":"ok","timestamp":1675331767395,"user_tz":-180,"elapsed":27886,"user":{"displayName":"Mahmoud Atia Ead Atia","userId":"11781797524477583668"}},"outputId":"edac2c2f-9525-4220-b1b7-07dc8ff45294"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-02-02 09:55:39--  https://www.dropbox.com/s/10bfat0kg4si1bu/zalando-hd-resized.zip\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.7.18, 2620:100:6019:18::a27d:412\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.7.18|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: /s/raw/10bfat0kg4si1bu/zalando-hd-resized.zip [following]\n","--2023-02-02 09:55:39--  https://www.dropbox.com/s/raw/10bfat0kg4si1bu/zalando-hd-resized.zip\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc9144f2d817fa1bc8ada4a3e897.dl.dropboxusercontent.com/cd/0/inline/B1vGbnAVXBkYT-hM83WzzjlQjzkX7_FoMuPZJGuam-zdlaxDXtGXW8nM9rniCUQXuVUlzHDQnTbG_smlVZ36Od7iUFl3KyWDD4uNpvvqIcaUs0_mgzpfPjdsDcGQYcN8MO7IpRQV6joQIYAe8DMe7omTL86uXBp_f9sj9Ijq-rIlbg/file# [following]\n","--2023-02-02 09:55:40--  https://uc9144f2d817fa1bc8ada4a3e897.dl.dropboxusercontent.com/cd/0/inline/B1vGbnAVXBkYT-hM83WzzjlQjzkX7_FoMuPZJGuam-zdlaxDXtGXW8nM9rniCUQXuVUlzHDQnTbG_smlVZ36Od7iUFl3KyWDD4uNpvvqIcaUs0_mgzpfPjdsDcGQYcN8MO7IpRQV6joQIYAe8DMe7omTL86uXBp_f9sj9Ijq-rIlbg/file\n","Resolving uc9144f2d817fa1bc8ada4a3e897.dl.dropboxusercontent.com (uc9144f2d817fa1bc8ada4a3e897.dl.dropboxusercontent.com)... 162.125.6.15, 2620:100:601c:15::a27d:60f\n","Connecting to uc9144f2d817fa1bc8ada4a3e897.dl.dropboxusercontent.com (uc9144f2d817fa1bc8ada4a3e897.dl.dropboxusercontent.com)|162.125.6.15|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: /cd/0/inline2/B1vtpTYc-xGtEv1USArEn6r_2EDldd9RwVUiYfYDKAWSZcZIQb0lMJqdLaGY8yAIozZpULH3jsGZ0yTd8tAB5QMTZy6lXmsvy1MFlv9JKeKFAH4ydZ2rTlMwQUe-QJkqy4tQdS5yqOXCP4FC1apIzCak7xP1x2A5BhD9yzkzVkS0B3h_Wxz8oZGED2CEPZTU4YQZSOgK_JuKnEJFP28wiadxG8YOBiOJx5iDqU8OFtowNhops0bjDH1d3SQtqQVBGTQCVhjo-dnTpsVmLnNFoK9VR-lqXDwJhnHEXBggRDuYKs8Cq0Yo_tc74xuhuP1GbZo73wjjggnJyqS7QAcNi45PneZo6I65XffiF-XRliRdzrtk1ftnLjRIJD6L9CaV7ykSDkjf31nAlwNlHIdMpprHVximVqgHBe1BUJGzEiANfw/file [following]\n","--2023-02-02 09:55:40--  https://uc9144f2d817fa1bc8ada4a3e897.dl.dropboxusercontent.com/cd/0/inline2/B1vtpTYc-xGtEv1USArEn6r_2EDldd9RwVUiYfYDKAWSZcZIQb0lMJqdLaGY8yAIozZpULH3jsGZ0yTd8tAB5QMTZy6lXmsvy1MFlv9JKeKFAH4ydZ2rTlMwQUe-QJkqy4tQdS5yqOXCP4FC1apIzCak7xP1x2A5BhD9yzkzVkS0B3h_Wxz8oZGED2CEPZTU4YQZSOgK_JuKnEJFP28wiadxG8YOBiOJx5iDqU8OFtowNhops0bjDH1d3SQtqQVBGTQCVhjo-dnTpsVmLnNFoK9VR-lqXDwJhnHEXBggRDuYKs8Cq0Yo_tc74xuhuP1GbZo73wjjggnJyqS7QAcNi45PneZo6I65XffiF-XRliRdzrtk1ftnLjRIJD6L9CaV7ykSDkjf31nAlwNlHIdMpprHVximVqgHBe1BUJGzEiANfw/file\n","Reusing existing connection to uc9144f2d817fa1bc8ada4a3e897.dl.dropboxusercontent.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4702718920 (4.4G) [application/zip]\n","Saving to: ‘zalando-hd-resized.zip’\n","\n","zalando-hd-resized. 100%[===================>]   4.38G   198MB/s    in 26s     \n","\n","2023-02-02 09:56:06 (174 MB/s) - ‘zalando-hd-resized.zip’ saved [4702718920/4702718920]\n","\n"]}]},{"cell_type":"code","source":["! unzip /content/zalando-hd-resized.zip"],"metadata":{"id":"SQKQRg-5i4El"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os.path as osp\n"],"metadata":{"id":"MFzMf8Ujw9TI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["opt = {\n","    \"root\":\"\",\n","    \"datamode\":\"train\",\n","    \"data_list\":\"train_pairs.txt\" ,\n","    \"fine_height\":1024,\n","    \"fine_width\" : 768,\n","    \"semantic\": 13 ,\n","    \"shuffle\" : False,\n","    \"batch_size\": 32,\n","    \"workers\":4,\n","    \"warp_feature\":\"T1\",\n","    \"out_layer\":\"relu\",\n","    \"output_nc\":13,\n","    \"datapath\": \"train\"\n","   }"],"metadata":{"id":"ONua4wBuuogb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset =  CPDatasetTest(opt)\n"],"metadata":{"id":"wi8iV9GaztaE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from PIL import Image\n","Image.open(\"'03059_00.jpg'\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":304},"id":"VxpujXD5Z-JJ","executionInfo":{"status":"error","timestamp":1675346104060,"user_tz":-180,"elapsed":422,"user":{"displayName":"Mahmoud Atia Ead Atia","userId":"11781797524477583668"}},"outputId":"284f0967-1f4f-4907-85aa-85b6f898f796"},"execution_count":151,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-151-798e57feee09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'03059_00.jpg'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2843\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2844\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: \"'03059_00.jpg'\""]}]},{"cell_type":"code","source":["dataset.im_names"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w0ZzqBAPZ5mr","executionInfo":{"status":"ok","timestamp":1675346032566,"user_tz":-180,"elapsed":9,"user":{"displayName":"Mahmoud Atia Ead Atia","userId":"11781797524477583668"}},"outputId":"149b918c-70c4-4f81-bb6e-7f231a921f37"},"execution_count":150,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['10224_00.jpg',\n"," '12308_00.jpg',\n"," '08020_00.jpg',\n"," '11066_00.jpg',\n"," '05020_00.jpg',\n"," '10814_00.jpg',\n"," '08994_00.jpg',\n"," '07690_00.jpg',\n"," '01492_00.jpg',\n"," '03456_00.jpg',\n"," '11772_00.jpg',\n"," '08282_00.jpg',\n"," '03395_00.jpg',\n"," '10520_00.jpg',\n"," '14603_00.jpg',\n"," '03373_00.jpg',\n"," '06852_00.jpg',\n"," '06848_00.jpg',\n"," '02418_00.jpg',\n"," '01279_00.jpg',\n"," '04869_00.jpg',\n"," '11233_00.jpg',\n"," '07660_00.jpg',\n"," '07498_00.jpg',\n"," '01600_00.jpg',\n"," '05969_00.jpg',\n"," '09497_00.jpg',\n"," '04124_00.jpg',\n"," '05895_00.jpg',\n"," '10385_00.jpg',\n"," '04959_00.jpg',\n"," '11031_00.jpg',\n"," '00799_00.jpg',\n"," '07320_00.jpg',\n"," '07415_00.jpg',\n"," '02888_00.jpg',\n"," '12999_00.jpg',\n"," '00136_00.jpg',\n"," '07783_00.jpg',\n"," '09567_00.jpg',\n"," '08623_00.jpg',\n"," '10521_00.jpg',\n"," '07422_00.jpg',\n"," '00304_00.jpg',\n"," '02630_00.jpg',\n"," '11721_00.jpg',\n"," '00318_00.jpg',\n"," '04460_00.jpg',\n"," '14443_00.jpg',\n"," '03028_00.jpg',\n"," '06859_00.jpg',\n"," '09611_00.jpg',\n"," '09381_00.jpg',\n"," '09376_00.jpg',\n"," '13294_00.jpg',\n"," '06427_00.jpg',\n"," '08275_00.jpg',\n"," '00704_00.jpg',\n"," '11490_00.jpg',\n"," '01746_00.jpg',\n"," '14665_00.jpg',\n"," '03432_00.jpg',\n"," '07174_00.jpg',\n"," '07032_00.jpg',\n"," '10232_00.jpg',\n"," '13359_00.jpg',\n"," '00661_00.jpg',\n"," '04070_00.jpg',\n"," '04229_00.jpg',\n"," '01144_00.jpg',\n"," '08861_00.jpg',\n"," '06915_00.jpg',\n"," '11685_00.jpg',\n"," '00778_00.jpg',\n"," '08026_00.jpg',\n"," '08276_00.jpg',\n"," '11963_00.jpg',\n"," '04567_00.jpg',\n"," '04500_00.jpg',\n"," '04744_00.jpg',\n"," '13886_00.jpg',\n"," '03423_00.jpg',\n"," '01125_00.jpg',\n"," '07472_00.jpg',\n"," '00673_00.jpg',\n"," '10752_00.jpg',\n"," '06916_00.jpg',\n"," '08204_00.jpg',\n"," '02410_00.jpg',\n"," '10295_00.jpg',\n"," '10536_00.jpg',\n"," '13067_00.jpg',\n"," '09669_00.jpg',\n"," '11457_00.jpg',\n"," '09825_00.jpg',\n"," '00191_00.jpg',\n"," '11256_00.jpg',\n"," '06782_00.jpg',\n"," '06791_00.jpg',\n"," '07265_00.jpg',\n"," '06938_00.jpg',\n"," '13376_00.jpg',\n"," '06335_00.jpg',\n"," '06676_00.jpg',\n"," '02085_00.jpg',\n"," '05818_00.jpg',\n"," '11255_00.jpg',\n"," '04036_00.jpg',\n"," '09974_00.jpg',\n"," '04902_00.jpg',\n"," '04900_00.jpg',\n"," '12016_00.jpg',\n"," '09037_00.jpg',\n"," '02282_00.jpg',\n"," '01826_00.jpg',\n"," '03144_00.jpg',\n"," '04249_00.jpg',\n"," '09806_00.jpg',\n"," '10975_00.jpg',\n"," '14642_00.jpg',\n"," '06909_00.jpg',\n"," '00572_00.jpg',\n"," '09956_00.jpg',\n"," '13775_00.jpg',\n"," '06772_00.jpg',\n"," '13134_00.jpg',\n"," '04109_00.jpg',\n"," '00003_00.jpg',\n"," '12405_00.jpg',\n"," '03748_00.jpg',\n"," '01784_00.jpg',\n"," '09447_00.jpg',\n"," '05113_00.jpg',\n"," '10833_00.jpg',\n"," '02559_00.jpg',\n"," '05484_00.jpg',\n"," '13889_00.jpg',\n"," '07989_00.jpg',\n"," '08550_00.jpg',\n"," '02479_00.jpg',\n"," '13817_00.jpg',\n"," '08934_00.jpg',\n"," '03164_00.jpg',\n"," '08583_00.jpg',\n"," '09719_00.jpg',\n"," '00089_00.jpg',\n"," '00724_00.jpg',\n"," '09211_00.jpg',\n"," '08640_00.jpg',\n"," '02369_00.jpg',\n"," '14437_00.jpg',\n"," '11692_00.jpg',\n"," '01946_00.jpg',\n"," '10670_00.jpg',\n"," '03282_00.jpg',\n"," '00465_00.jpg',\n"," '13122_00.jpg',\n"," '04634_00.jpg',\n"," '00228_00.jpg',\n"," '05389_00.jpg',\n"," '11123_00.jpg',\n"," '13738_00.jpg',\n"," '01151_00.jpg',\n"," '07701_00.jpg',\n"," '02756_00.jpg',\n"," '04203_00.jpg',\n"," '04087_00.jpg',\n"," '08492_00.jpg',\n"," '05133_00.jpg',\n"," '01773_00.jpg',\n"," '12682_00.jpg',\n"," '13821_00.jpg',\n"," '13334_00.jpg',\n"," '02950_00.jpg',\n"," '10243_00.jpg',\n"," '13724_00.jpg',\n"," '14338_00.jpg',\n"," '04466_00.jpg',\n"," '09695_00.jpg',\n"," '04279_00.jpg',\n"," '01710_00.jpg',\n"," '07337_00.jpg',\n"," '10605_00.jpg',\n"," '11821_00.jpg',\n"," '03185_00.jpg',\n"," '06197_00.jpg',\n"," '06556_00.jpg',\n"," '06844_00.jpg',\n"," '13182_00.jpg',\n"," '10341_00.jpg',\n"," '04225_00.jpg',\n"," '11801_00.jpg',\n"," '02700_00.jpg',\n"," '00794_00.jpg',\n"," '05827_00.jpg',\n"," '00073_00.jpg',\n"," '13907_00.jpg',\n"," '09943_00.jpg',\n"," '07625_00.jpg',\n"," '13774_00.jpg',\n"," '02065_00.jpg',\n"," '12970_00.jpg',\n"," '00118_00.jpg',\n"," '01460_00.jpg',\n"," '04575_00.jpg',\n"," '04573_00.jpg',\n"," '12269_00.jpg',\n"," '08783_00.jpg',\n"," '09982_00.jpg',\n"," '00232_00.jpg',\n"," '09986_00.jpg',\n"," '09126_00.jpg',\n"," '03869_00.jpg',\n"," '12610_00.jpg',\n"," '08240_00.jpg',\n"," '06830_00.jpg',\n"," '02322_00.jpg',\n"," '09007_00.jpg',\n"," '14137_00.jpg',\n"," '14502_00.jpg',\n"," '06012_00.jpg',\n"," '10602_00.jpg',\n"," '05391_00.jpg',\n"," '11890_00.jpg',\n"," '06579_00.jpg',\n"," '08065_00.jpg',\n"," '04340_00.jpg',\n"," '03203_00.jpg',\n"," '08476_00.jpg',\n"," '11371_00.jpg',\n"," '12933_00.jpg',\n"," '13764_00.jpg',\n"," '01538_00.jpg',\n"," '03687_00.jpg',\n"," '08546_00.jpg',\n"," '10627_00.jpg',\n"," '13999_00.jpg',\n"," '02522_00.jpg',\n"," '06404_00.jpg',\n"," '02529_00.jpg',\n"," '13135_00.jpg',\n"," '02170_00.jpg',\n"," '13424_00.jpg',\n"," '09075_00.jpg',\n"," '02393_00.jpg',\n"," '11072_00.jpg',\n"," '02368_00.jpg',\n"," '14267_00.jpg',\n"," '02850_00.jpg',\n"," '03149_00.jpg',\n"," '02766_00.jpg',\n"," '03995_00.jpg',\n"," '12209_00.jpg',\n"," '10367_00.jpg',\n"," '06583_00.jpg',\n"," '06557_00.jpg',\n"," '13785_00.jpg',\n"," '02086_00.jpg',\n"," '13614_00.jpg',\n"," '10883_00.jpg',\n"," '12112_00.jpg',\n"," '11090_00.jpg',\n"," '13870_00.jpg',\n"," '02035_00.jpg',\n"," '02143_00.jpg',\n"," '11452_00.jpg',\n"," '06014_00.jpg',\n"," '06826_00.jpg',\n"," '02135_00.jpg',\n"," '08336_00.jpg',\n"," '11971_00.jpg',\n"," '10429_00.jpg',\n"," '06357_00.jpg',\n"," '09495_00.jpg',\n"," '13828_00.jpg',\n"," '07096_00.jpg',\n"," '09878_00.jpg',\n"," '04717_00.jpg',\n"," '00556_00.jpg',\n"," '01660_00.jpg',\n"," '00534_00.jpg',\n"," '13887_00.jpg',\n"," '13188_00.jpg',\n"," '09721_00.jpg',\n"," '00025_00.jpg',\n"," '03703_00.jpg',\n"," '04316_00.jpg',\n"," '13725_00.jpg',\n"," '05939_00.jpg',\n"," '14427_00.jpg',\n"," '08798_00.jpg',\n"," '07231_00.jpg',\n"," '11137_00.jpg',\n"," '00507_00.jpg',\n"," '10468_00.jpg',\n"," '12140_00.jpg',\n"," '07507_00.jpg',\n"," '07893_00.jpg',\n"," '13832_00.jpg',\n"," '10096_00.jpg',\n"," '00825_00.jpg',\n"," '08506_00.jpg',\n"," '11681_00.jpg',\n"," '01810_00.jpg',\n"," '05283_00.jpg',\n"," '00757_00.jpg',\n"," '03539_00.jpg',\n"," '13451_00.jpg',\n"," '13841_00.jpg',\n"," '12573_00.jpg',\n"," '03031_00.jpg',\n"," '06674_00.jpg',\n"," '13298_00.jpg',\n"," '02309_00.jpg',\n"," '00102_00.jpg',\n"," '03886_00.jpg',\n"," '03837_00.jpg',\n"," '00211_00.jpg',\n"," '03259_00.jpg',\n"," '05222_00.jpg',\n"," '07125_00.jpg',\n"," '10181_00.jpg',\n"," '09377_00.jpg',\n"," '06609_00.jpg',\n"," '12245_00.jpg',\n"," '10361_00.jpg',\n"," '03631_00.jpg',\n"," '04511_00.jpg',\n"," '01233_00.jpg',\n"," '06850_00.jpg',\n"," '03815_00.jpg',\n"," '03420_00.jpg',\n"," '03795_00.jpg',\n"," '03170_00.jpg',\n"," '12772_00.jpg',\n"," '07478_00.jpg',\n"," '07866_00.jpg',\n"," '10539_00.jpg',\n"," '12417_00.jpg',\n"," '04846_00.jpg',\n"," '04364_00.jpg',\n"," '00843_00.jpg',\n"," '02076_00.jpg',\n"," '13782_00.jpg',\n"," '04007_00.jpg',\n"," '04513_00.jpg',\n"," '11654_00.jpg',\n"," '05355_00.jpg',\n"," '11644_00.jpg',\n"," '02822_00.jpg',\n"," '00910_00.jpg',\n"," '10678_00.jpg',\n"," '04346_00.jpg',\n"," '10392_00.jpg',\n"," '10044_00.jpg',\n"," '00819_00.jpg',\n"," '08626_00.jpg',\n"," '10667_00.jpg',\n"," '08121_00.jpg',\n"," '03293_00.jpg',\n"," '01103_00.jpg',\n"," '05754_00.jpg',\n"," '06507_00.jpg',\n"," '05899_00.jpg',\n"," '07436_00.jpg',\n"," '05267_00.jpg',\n"," '08153_00.jpg',\n"," '08854_00.jpg',\n"," '12516_00.jpg',\n"," '02068_00.jpg',\n"," '12687_00.jpg',\n"," '10593_00.jpg',\n"," '05492_00.jpg',\n"," '03503_00.jpg',\n"," '08096_00.jpg',\n"," '11367_00.jpg',\n"," '03141_00.jpg',\n"," '12028_00.jpg',\n"," '10530_00.jpg',\n"," '03706_00.jpg',\n"," '10642_00.jpg',\n"," '11381_00.jpg',\n"," '14423_00.jpg',\n"," '14428_00.jpg',\n"," '03179_00.jpg',\n"," '11006_00.jpg',\n"," '08235_00.jpg',\n"," '10722_00.jpg',\n"," '02146_00.jpg',\n"," '05481_00.jpg',\n"," '11203_00.jpg',\n"," '13679_00.jpg',\n"," '13388_00.jpg',\n"," '07550_00.jpg',\n"," '12892_00.jpg',\n"," '04808_00.jpg',\n"," '10508_00.jpg',\n"," '05951_00.jpg',\n"," '12296_00.jpg',\n"," '06013_00.jpg',\n"," '01775_00.jpg',\n"," '03055_00.jpg',\n"," '07486_00.jpg',\n"," '14635_00.jpg',\n"," '13581_00.jpg',\n"," '04085_00.jpg',\n"," '02618_00.jpg',\n"," '11850_00.jpg',\n"," '09499_00.jpg',\n"," '11049_00.jpg',\n"," '14012_00.jpg',\n"," '01153_00.jpg',\n"," '04660_00.jpg',\n"," '05666_00.jpg',\n"," '01730_00.jpg',\n"," '12804_00.jpg',\n"," '07940_00.jpg',\n"," '01972_00.jpg',\n"," '02350_00.jpg',\n"," '12545_00.jpg',\n"," '14373_00.jpg',\n"," '11486_00.jpg',\n"," '11314_00.jpg',\n"," '02121_00.jpg',\n"," '09994_00.jpg',\n"," '13415_00.jpg',\n"," '10472_00.jpg',\n"," '06118_00.jpg',\n"," '10881_00.jpg',\n"," '04616_00.jpg',\n"," '09209_00.jpg',\n"," '02825_00.jpg',\n"," '00218_00.jpg',\n"," '13237_00.jpg',\n"," '01866_00.jpg',\n"," '13612_00.jpg',\n"," '01752_00.jpg',\n"," '12623_00.jpg',\n"," '01138_00.jpg',\n"," '10841_00.jpg',\n"," '00741_00.jpg',\n"," '11953_00.jpg',\n"," '12292_00.jpg',\n"," '02227_00.jpg',\n"," '08528_00.jpg',\n"," '01496_00.jpg',\n"," '11263_00.jpg',\n"," '03165_00.jpg',\n"," '09347_00.jpg',\n"," '14650_00.jpg',\n"," '04805_00.jpg',\n"," '02335_00.jpg',\n"," '13362_00.jpg',\n"," '14125_00.jpg',\n"," '02488_00.jpg',\n"," '00032_00.jpg',\n"," '14647_00.jpg',\n"," '04520_00.jpg',\n"," '03522_00.jpg',\n"," '10803_00.jpg',\n"," '11496_00.jpg',\n"," '03947_00.jpg',\n"," '00041_00.jpg',\n"," '05171_00.jpg',\n"," '06731_00.jpg',\n"," '02430_00.jpg',\n"," '04309_00.jpg',\n"," '05521_00.jpg',\n"," '06346_00.jpg',\n"," '01829_00.jpg',\n"," '06209_00.jpg',\n"," '09386_00.jpg',\n"," '14343_00.jpg',\n"," '11667_00.jpg',\n"," '14598_00.jpg',\n"," '08775_00.jpg',\n"," '02644_00.jpg',\n"," '13003_00.jpg',\n"," '14602_00.jpg',\n"," '13506_00.jpg',\n"," '01467_00.jpg',\n"," '06090_00.jpg',\n"," '11183_00.jpg',\n"," '01512_00.jpg',\n"," '06436_00.jpg',\n"," '07815_00.jpg',\n"," '01539_00.jpg',\n"," '01040_00.jpg',\n"," '14077_00.jpg',\n"," '04102_00.jpg',\n"," '10252_00.jpg',\n"," '07037_00.jpg',\n"," '13143_00.jpg',\n"," '05670_00.jpg',\n"," '03757_00.jpg',\n"," '09643_00.jpg',\n"," '08116_00.jpg',\n"," '09573_00.jpg',\n"," '09323_00.jpg',\n"," '13921_00.jpg',\n"," '07304_00.jpg',\n"," '01378_00.jpg',\n"," '12722_00.jpg',\n"," '00361_00.jpg',\n"," '04257_00.jpg',\n"," '13861_00.jpg',\n"," '06540_00.jpg',\n"," '11895_00.jpg',\n"," '08387_00.jpg',\n"," '11071_00.jpg',\n"," '02866_00.jpg',\n"," '04439_00.jpg',\n"," '03667_00.jpg',\n"," '04901_00.jpg',\n"," '04898_00.jpg',\n"," '12449_00.jpg',\n"," '06599_00.jpg',\n"," '07042_00.jpg',\n"," '13027_00.jpg',\n"," '04755_00.jpg',\n"," '00319_00.jpg',\n"," '04063_00.jpg',\n"," '13421_00.jpg',\n"," '08992_00.jpg',\n"," '10433_00.jpg',\n"," '11538_00.jpg',\n"," '09612_00.jpg',\n"," '13227_00.jpg',\n"," '10970_00.jpg',\n"," '08659_00.jpg',\n"," '11339_00.jpg',\n"," '04526_00.jpg',\n"," '02903_00.jpg',\n"," '08398_00.jpg',\n"," '00883_00.jpg',\n"," '14396_00.jpg',\n"," '03965_00.jpg',\n"," '14515_00.jpg',\n"," '12575_00.jpg',\n"," '03021_00.jpg',\n"," '07354_00.jpg',\n"," '11419_00.jpg',\n"," '05896_00.jpg',\n"," '10060_00.jpg',\n"," '01654_00.jpg',\n"," '05977_00.jpg',\n"," '06651_00.jpg',\n"," '09452_00.jpg',\n"," '08801_00.jpg',\n"," '03269_00.jpg',\n"," '01497_00.jpg',\n"," '10180_00.jpg',\n"," '10275_00.jpg',\n"," '01088_00.jpg',\n"," '06080_00.jpg',\n"," '06524_00.jpg',\n"," '12949_00.jpg',\n"," '12216_00.jpg',\n"," '07397_00.jpg',\n"," '03762_00.jpg',\n"," '12939_00.jpg',\n"," '12434_00.jpg',\n"," '01342_00.jpg',\n"," '07915_00.jpg',\n"," '05689_00.jpg',\n"," '02643_00.jpg',\n"," '04638_00.jpg',\n"," '02949_00.jpg',\n"," '02626_00.jpg',\n"," '08618_00.jpg',\n"," '04838_00.jpg',\n"," '12025_00.jpg',\n"," '03441_00.jpg',\n"," '02789_00.jpg',\n"," '08698_00.jpg',\n"," '03810_00.jpg',\n"," '00917_00.jpg',\n"," '00199_00.jpg',\n"," '07493_00.jpg',\n"," '00501_00.jpg',\n"," '14283_00.jpg',\n"," '07220_00.jpg',\n"," '01848_00.jpg',\n"," '05116_00.jpg',\n"," '06987_00.jpg',\n"," '08473_00.jpg',\n"," '07850_00.jpg',\n"," '12139_00.jpg',\n"," '13805_00.jpg',\n"," '02453_00.jpg',\n"," '13803_00.jpg',\n"," '12151_00.jpg',\n"," '07632_00.jpg',\n"," '04306_00.jpg',\n"," '08576_00.jpg',\n"," '06656_00.jpg',\n"," '04440_00.jpg',\n"," '13600_00.jpg',\n"," '07226_00.jpg',\n"," '01639_00.jpg',\n"," '03563_00.jpg',\n"," '02556_00.jpg',\n"," '01655_00.jpg',\n"," '03281_00.jpg',\n"," '05076_00.jpg',\n"," '02230_00.jpg',\n"," '02696_00.jpg',\n"," '12968_00.jpg',\n"," '04368_00.jpg',\n"," '04223_00.jpg',\n"," '00377_00.jpg',\n"," '06084_00.jpg',\n"," '03843_00.jpg',\n"," '13671_00.jpg',\n"," '03352_00.jpg',\n"," '06206_00.jpg',\n"," '02107_00.jpg',\n"," '10360_00.jpg',\n"," '07719_00.jpg',\n"," '13114_00.jpg',\n"," '06720_00.jpg',\n"," '09547_00.jpg',\n"," '07392_00.jpg',\n"," '03213_00.jpg',\n"," '00636_00.jpg',\n"," '08855_00.jpg',\n"," '00398_00.jpg',\n"," '03760_00.jpg',\n"," '00831_00.jpg',\n"," '12930_00.jpg',\n"," '04673_00.jpg',\n"," '09958_00.jpg',\n"," '10270_00.jpg',\n"," '05763_00.jpg',\n"," '05311_00.jpg',\n"," '08639_00.jpg',\n"," '02841_00.jpg',\n"," '14155_00.jpg',\n"," '01698_00.jpg',\n"," '12445_00.jpg',\n"," '11096_00.jpg',\n"," '10095_00.jpg',\n"," '14145_00.jpg',\n"," '06999_00.jpg',\n"," '08714_00.jpg',\n"," '07091_00.jpg',\n"," '13503_00.jpg',\n"," '09622_00.jpg',\n"," '03090_00.jpg',\n"," '12662_00.jpg',\n"," '12229_00.jpg',\n"," '10765_00.jpg',\n"," '12440_00.jpg',\n"," '10903_00.jpg',\n"," '00335_00.jpg',\n"," '01315_00.jpg',\n"," '06917_00.jpg',\n"," '03825_00.jpg',\n"," '06933_00.jpg',\n"," '00667_00.jpg',\n"," '12683_00.jpg',\n"," '09127_00.jpg',\n"," '00282_00.jpg',\n"," '03059_00.jpg',\n"," '11473_00.jpg',\n"," '00111_00.jpg',\n"," '09072_00.jpg',\n"," '14143_00.jpg',\n"," '03097_00.jpg',\n"," '12226_00.jpg',\n"," '04558_00.jpg',\n"," '11601_00.jpg',\n"," '00966_00.jpg',\n"," '01227_00.jpg',\n"," '02480_00.jpg',\n"," '14082_00.jpg',\n"," '06764_00.jpg',\n"," '05800_00.jpg',\n"," '00476_00.jpg',\n"," '08479_00.jpg',\n"," '11154_00.jpg',\n"," '06942_00.jpg',\n"," '01422_00.jpg',\n"," '07712_00.jpg',\n"," '00671_00.jpg',\n"," '09370_00.jpg',\n"," '02539_00.jpg',\n"," '08066_00.jpg',\n"," '08343_00.jpg',\n"," '01680_00.jpg',\n"," '13575_00.jpg',\n"," '01990_00.jpg',\n"," '06377_00.jpg',\n"," '09861_00.jpg',\n"," '06996_00.jpg',\n"," '12910_00.jpg',\n"," '09717_00.jpg',\n"," '02746_00.jpg',\n"," '00210_00.jpg',\n"," '05080_00.jpg',\n"," '00364_00.jpg',\n"," '00192_00.jpg',\n"," '04916_00.jpg',\n"," '05909_00.jpg',\n"," '14527_00.jpg',\n"," '05894_00.jpg',\n"," '12843_00.jpg',\n"," '05733_00.jpg',\n"," '12307_00.jpg',\n"," '03233_00.jpg',\n"," '06367_00.jpg',\n"," '09516_00.jpg',\n"," '01608_00.jpg',\n"," '13311_00.jpg',\n"," '03019_00.jpg',\n"," '01873_00.jpg',\n"," '06182_00.jpg',\n"," '02301_00.jpg',\n"," '03537_00.jpg',\n"," '11585_00.jpg',\n"," '12769_00.jpg',\n"," '13255_00.jpg',\n"," '08541_00.jpg',\n"," '12103_00.jpg',\n"," '08036_00.jpg',\n"," '00626_00.jpg',\n"," '02802_00.jpg',\n"," '00511_00.jpg',\n"," '07379_00.jpg',\n"," '12483_00.jpg',\n"," '00417_00.jpg',\n"," '04890_00.jpg',\n"," '12664_00.jpg',\n"," '06903_00.jpg',\n"," '05787_00.jpg',\n"," '00923_00.jpg',\n"," '01435_00.jpg',\n"," '05487_00.jpg',\n"," '06142_00.jpg',\n"," '11243_00.jpg',\n"," '12182_00.jpg',\n"," '12507_00.jpg',\n"," '09344_00.jpg',\n"," '00464_00.jpg',\n"," '06074_00.jpg',\n"," '06706_00.jpg',\n"," '03155_00.jpg',\n"," '14371_00.jpg',\n"," '01394_00.jpg',\n"," '03049_00.jpg',\n"," '11833_00.jpg',\n"," '06495_00.jpg',\n"," '09131_00.jpg',\n"," '12751_00.jpg',\n"," '14649_00.jpg',\n"," '00833_00.jpg',\n"," '14385_00.jpg',\n"," '09833_00.jpg',\n"," '04907_00.jpg',\n"," '04568_00.jpg',\n"," '04129_00.jpg',\n"," '11743_00.jpg',\n"," '04437_00.jpg',\n"," '03323_00.jpg',\n"," '09688_00.jpg',\n"," '02000_00.jpg',\n"," '03494_00.jpg',\n"," '14435_00.jpg',\n"," '07139_00.jpg',\n"," '03617_00.jpg',\n"," '11048_00.jpg',\n"," '13200_00.jpg',\n"," '08540_00.jpg',\n"," '12853_00.jpg',\n"," '04999_00.jpg',\n"," '09758_00.jpg',\n"," '01939_00.jpg',\n"," '07698_00.jpg',\n"," '08828_00.jpg',\n"," '03662_00.jpg',\n"," '13295_00.jpg',\n"," '06602_00.jpg',\n"," '09604_00.jpg',\n"," '04181_00.jpg',\n"," '03904_00.jpg',\n"," '06136_00.jpg',\n"," '00374_00.jpg',\n"," '00367_00.jpg',\n"," '04195_00.jpg',\n"," '13794_00.jpg',\n"," '06283_00.jpg',\n"," '10269_00.jpg',\n"," '07774_00.jpg',\n"," '08058_00.jpg',\n"," '02286_00.jpg',\n"," '07538_00.jpg',\n"," '13838_00.jpg',\n"," '05198_00.jpg',\n"," '10463_00.jpg',\n"," '10745_00.jpg',\n"," '13325_00.jpg',\n"," '07650_00.jpg',\n"," '00105_00.jpg',\n"," '13845_00.jpg',\n"," '03930_00.jpg',\n"," '06255_00.jpg',\n"," '04033_00.jpg',\n"," '13837_00.jpg',\n"," '07720_00.jpg',\n"," '04281_00.jpg',\n"," '12880_00.jpg',\n"," '05397_00.jpg',\n"," '03774_00.jpg',\n"," '08637_00.jpg',\n"," '11831_00.jpg',\n"," '02496_00.jpg',\n"," '03575_00.jpg',\n"," '08029_00.jpg',\n"," '12208_00.jpg',\n"," '02986_00.jpg',\n"," '09088_00.jpg',\n"," '06885_00.jpg',\n"," '11790_00.jpg',\n"," '00315_00.jpg',\n"," '14324_00.jpg',\n"," '09365_00.jpg',\n"," '06246_00.jpg',\n"," '09462_00.jpg',\n"," '13130_00.jpg',\n"," '06421_00.jpg',\n"," '07992_00.jpg',\n"," '07136_00.jpg',\n"," '09483_00.jpg',\n"," '06438_00.jpg',\n"," '09712_00.jpg',\n"," '10607_00.jpg',\n"," '14163_00.jpg',\n"," '07695_00.jpg',\n"," '00580_00.jpg',\n"," '13660_00.jpg',\n"," '03971_00.jpg',\n"," '06821_00.jpg',\n"," '04930_00.jpg',\n"," '00795_00.jpg',\n"," '14398_00.jpg',\n"," '05407_00.jpg',\n"," '09607_00.jpg',\n"," '14115_00.jpg',\n"," '04227_00.jpg',\n"," '01345_00.jpg',\n"," '12015_00.jpg',\n"," '01033_00.jpg',\n"," '00885_00.jpg',\n"," '04158_00.jpg',\n"," '06843_00.jpg',\n"," '12225_00.jpg',\n"," '01375_00.jpg',\n"," '02145_00.jpg',\n"," '05999_00.jpg',\n"," '12463_00.jpg',\n"," '06220_00.jpg',\n"," '14526_00.jpg',\n"," '11135_00.jpg',\n"," '05134_00.jpg',\n"," '07135_00.jpg',\n"," '01284_00.jpg',\n"," '07973_00.jpg',\n"," '01789_00.jpg',\n"," '06342_00.jpg',\n"," '06077_00.jpg',\n"," '07081_00.jpg',\n"," '09724_00.jpg',\n"," '05319_00.jpg',\n"," '10837_00.jpg',\n"," '04645_00.jpg',\n"," '07519_00.jpg',\n"," '02817_00.jpg',\n"," '14219_00.jpg',\n"," '06001_00.jpg',\n"," '07750_00.jpg',\n"," '11190_00.jpg',\n"," '11344_00.jpg',\n"," '03994_00.jpg',\n"," '08107_00.jpg',\n"," '08224_00.jpg',\n"," '02965_00.jpg',\n"," '00577_00.jpg',\n"," '12479_00.jpg',\n"," '01213_00.jpg',\n"," '01903_00.jpg',\n"," '11305_00.jpg',\n"," '11981_00.jpg',\n"," '08339_00.jpg',\n"," '07613_00.jpg',\n"," '09976_00.jpg',\n"," '13038_00.jpg',\n"," '04363_00.jpg',\n"," '01694_00.jpg',\n"," '09796_00.jpg',\n"," '11165_00.jpg',\n"," '06701_00.jpg',\n"," '08868_00.jpg',\n"," '05168_00.jpg',\n"," '09460_00.jpg',\n"," '12153_00.jpg',\n"," '12204_00.jpg',\n"," '05096_00.jpg',\n"," '14319_00.jpg',\n"," '00065_00.jpg',\n"," '13072_00.jpg',\n"," '08603_00.jpg',\n"," '11977_00.jpg',\n"," '01657_00.jpg',\n"," '07823_00.jpg',\n"," '05651_00.jpg',\n"," '07058_00.jpg',\n"," '10018_00.jpg',\n"," '00183_00.jpg',\n"," '14682_00.jpg',\n"," '10299_00.jpg',\n"," '05880_00.jpg',\n"," '09274_00.jpg',\n"," '02510_00.jpg',\n"," '11848_00.jpg',\n"," '08747_00.jpg',\n"," '13852_00.jpg',\n"," '04362_00.jpg',\n"," '11446_00.jpg',\n"," '10053_00.jpg',\n"," '06048_00.jpg',\n"," '07062_00.jpg',\n"," '08040_00.jpg',\n"," '07215_00.jpg',\n"," '05823_00.jpg',\n"," '09471_00.jpg',\n"," '04055_00.jpg',\n"," '07512_00.jpg',\n"," '12522_00.jpg',\n"," '03387_00.jpg',\n"," '02308_00.jpg',\n"," '06378_00.jpg',\n"," '08436_00.jpg',\n"," '12701_00.jpg',\n"," '11524_00.jpg',\n"," '12941_00.jpg',\n"," '03309_00.jpg',\n"," '02655_00.jpg',\n"," '11347_00.jpg',\n"," '05527_00.jpg',\n"," '09202_00.jpg',\n"," '05410_00.jpg',\n"," '07846_00.jpg',\n"," '11976_00.jpg',\n"," '06934_00.jpg',\n"," '04542_00.jpg',\n"," '02260_00.jpg',\n"," '13464_00.jpg',\n"," '02161_00.jpg',\n"," '08889_00.jpg',\n"," '04406_00.jpg',\n"," '14135_00.jpg',\n"," '07607_00.jpg',\n"," '11188_00.jpg',\n"," '07222_00.jpg',\n"," '03263_00.jpg',\n"," '03437_00.jpg',\n"," '10850_00.jpg',\n"," '02695_00.jpg',\n"," '12977_00.jpg',\n"," '01448_00.jpg',\n"," '10141_00.jpg',\n"," '06434_00.jpg',\n"," '08509_00.jpg',\n"," '14192_00.jpg',\n"," '08843_00.jpg',\n"," '05611_00.jpg',\n"," '04467_00.jpg',\n"," '07651_00.jpg',\n"," '10129_00.jpg',\n"," '14057_00.jpg',\n"," '02865_00.jpg',\n"," '08478_00.jpg',\n"," '02192_00.jpg',\n"," '10072_00.jpg',\n"," '09322_00.jpg',\n"," '10561_00.jpg',\n"," '04194_00.jpg',\n"," '10234_00.jpg',\n"," '13217_00.jpg',\n"," '06482_00.jpg',\n"," '11893_00.jpg',\n"," '10787_00.jpg',\n"," '00171_00.jpg',\n"," '09815_00.jpg',\n"," '00941_00.jpg',\n"," '11928_00.jpg',\n"," '02540_00.jpg',\n"," '02735_00.jpg',\n"," '10817_00.jpg',\n"," '07207_00.jpg',\n"," ...]"]},"metadata":{},"execution_count":150}]},{"cell_type":"code","source":["torch.utils.data.DataLoader(\n","                dataset, batch_size=opt.batch_size, shuffle=(train_sampler is None),\n","                num_workers=opt.workers, pin_memory=True, drop_last=True, sampler=train_sampler)\n","        self.dataset = dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":130},"id":"6VxvjN3gI510","executionInfo":{"status":"error","timestamp":1675342404177,"user_tz":-180,"elapsed":350,"user":{"displayName":"Mahmoud Atia Ead Atia","userId":"11781797524477583668"}},"outputId":"eb831dfe-a960-4eb2-c1ec-567e8ccde89e"},"execution_count":null,"outputs":[{"output_type":"error","ename":"IndentationError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-142-1037806f3f79>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    self.dataset = dataset\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"]}]},{"cell_type":"code","source":["dataset.name()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"WisFKEaMKHg0","executionInfo":{"status":"ok","timestamp":1675342407375,"user_tz":-180,"elapsed":259,"user":{"displayName":"Mahmoud Atia Ead Atia","userId":"11781797524477583668"}},"outputId":"ce86de0b-d2f8-4914-9d42-502e5ef8d48b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'CPDataset'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":143}]},{"cell_type":"code","source":["dl = CPDataLoader(opt,r)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oRQ6lUtz1iAS","executionInfo":{"status":"ok","timestamp":1675342413627,"user_tz":-180,"elapsed":266,"user":{"displayName":"Mahmoud Atia Ead Atia","userId":"11781797524477583668"}},"outputId":"98e7e48a-326c-4464-8138-e83f4c4a21d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}]},{"cell_type":"code","source":["dl.next_batch()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":606},"id":"ozJDDji-HkK1","executionInfo":{"status":"error","timestamp":1675342430306,"user_tz":-180,"elapsed":789,"user":{"displayName":"Mahmoud Atia Ead Atia","userId":"11781797524477583668"}},"outputId":"d3d5ad3c-ecf5-47cd-e741-d06d00ad9b6d"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-145-8949b47f5a05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-135-58cd6ff508f5>\u001b[0m in \u001b[0;36mnext_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_iter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1331\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1333\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1357\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1360\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-46-e5ec71269359>\", line 121, in __getitem__\n    c[key] = Image.open(osp.join(self.data_path, 'cloth', c_name[key])).convert('RGB')\n  File \"/usr/local/lib/python3.8/dist-packages/PIL/Image.py\", line 2843, in open\n    fp = builtins.open(filename, \"rb\")\nFileNotFoundError: [Errno 2] No such file or directory: '/content/test/cloth/03472_00.jpg'\n"]}]},{"cell_type":"code","source":[" train_sampler = torch.utils.data.sampler.RandomSampler(r)\n"],"metadata":{"id":"Ov_S9q1w6NPu","executionInfo":{"status":"ok","timestamp":1675346005857,"user_tz":-180,"elapsed":10,"user":{"displayName":"Mahmoud Atia Ead Atia","userId":"11781797524477583668"}}},"execution_count":146,"outputs":[]},{"cell_type":"code","source":["from torch import nn"],"metadata":{"id":"tk6ax1Fh9KRI","executionInfo":{"status":"ok","timestamp":1675346005858,"user_tz":-180,"elapsed":9,"user":{"displayName":"Mahmoud Atia Ead Atia","userId":"11781797524477583668"}}},"execution_count":147,"outputs":[]},{"cell_type":"code","source":["input1_nc = 4  # cloth + cloth-mask\n","input2_nc = opt[\"semantic\"] + 3  # parse_agnostic + densepose\n","tocg = ConditionGenerator(opt, input1_nc=input1_nc, input2_nc=input2_nc, output_nc=opt[\"output_nc\"], ngf=96, norm_layer=nn.BatchNorm2d)\n","       "],"metadata":{"id":"t84qPzZN6n0D","executionInfo":{"status":"ok","timestamp":1675346007180,"user_tz":-180,"elapsed":1330,"user":{"displayName":"Mahmoud Atia Ead Atia","userId":"11781797524477583668"}}},"execution_count":148,"outputs":[]},{"cell_type":"code","source":["tocg.forward(opt, input1=input1_nc, input2_=input2_nc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":165},"id":"zni3JU4t-MfY","executionInfo":{"status":"error","timestamp":1675346007181,"user_tz":-180,"elapsed":12,"user":{"displayName":"Mahmoud Atia Ead Atia","userId":"11781797524477583668"}},"outputId":"9fa1d68a-208a-4838-d0e1-6aaf8f203312"},"execution_count":149,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-149-88afcbdd6bcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtocg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput1_nc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput2_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput2_nc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'input2_'"]}]},{"cell_type":"code","source":[" with torch.no_grad():\n","        for inputs in l.data_loader:\n","\n","      \n","            pose_map = inputs['pose']\n","            pre_clothes_mask = inputs['cloth_mask'][opt.datasetting]\n","            label = inputs['parse']\n","            parse_agnostic = inputs['parse_agnostic']\n","            agnostic = inputs['agnostic']\n","            clothes = inputs['cloth'][opt.datasetting] # target cloth\n","            densepose = inputs['densepose']\n","            im = inputs['image']\n","            input_label, input_parse_agnostic = label, parse_agnostic\n","            pre_clothes_mask = torch.FloatTensor((pre_clothes_mask.detach().cpu().numpy() > 0.5).astype(np.float))\n","\n","\n","\n","            # down\n","            pose_map_down = F.interpolate(pose_map, size=(256, 192), mode='bilinear')\n","            pre_clothes_mask_down = F.interpolate(pre_clothes_mask, size=(256, 192), mode='nearest')\n","            input_label_down = F.interpolate(input_label, size=(256, 192), mode='bilinear')\n","            input_parse_agnostic_down = F.interpolate(input_parse_agnostic, size=(256, 192), mode='nearest')\n","            agnostic_down = F.interpolate(agnostic, size=(256, 192), mode='nearest')\n","            clothes_down = F.interpolate(clothes, size=(256, 192), mode='bilinear')\n","            densepose_down = F.interpolate(densepose, size=(256, 192), mode='bilinear')\n","\n","            shape = pre_clothes_mask.shape\n","            \n","            # multi-task inputs\n","            input1 = torch.cat([clothes_down, pre_clothes_mask_down], 1)\n","            input2 = torch.cat([input_parse_agnostic_down, densepose_down], 1)\n","\n","            # forward\n","            flow_list, fake_segmap, warped_cloth_paired, warped_clothmask_paired = tocg(opt,input1, input2)\n","\n","            break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"TderWl6GBHFP","executionInfo":{"status":"error","timestamp":1675347181010,"user_tz":-180,"elapsed":40317,"user":{"displayName":"Mahmoud Atia Ead Atia","userId":"11781797524477583668"}},"outputId":"90a58be4-fa65-49c5-9fbb-9e7578a28840"},"execution_count":155,"outputs":[{"output_type":"stream","name":"stderr","text":["Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f99a55cbee0><function _MultiProcessingDataLoaderIter.__del__ at 0x7f99a55cbee0>Exception ignored in: \n","Traceback (most recent call last):\n","\n","<function _MultiProcessingDataLoaderIter.__del__ at 0x7f99a55cbee0>  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n","Traceback (most recent call last):\n","\n","Exception ignored in:       File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n","self._shutdown_workers()<function _MultiProcessingDataLoaderIter.__del__ at 0x7f99a55cbee0>\n","Traceback (most recent call last):\n","\n","    Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n","self._shutdown_workers()          File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n","\n","if w.is_alive():self._shutdown_workers()    \n","\n","    self._shutdown_workers()  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n","if w.is_alive():\n","\n","      File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n","      File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n","assert self._parent_pid == os.getpid(), 'can only test a child process'\n","    if w.is_alive():    if w.is_alive():assert self._parent_pid == os.getpid(), 'can only test a child process'    AssertionError\n","AssertionError: \n","  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n","\n","assert self._parent_pid == os.getpid(), 'can only test a child process':   File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n","can only test a child process    \n","can only test a child process\n","assert self._parent_pid == os.getpid(), 'can only test a child process'\n","AssertionErrorAssertionError: \n","Exception ignored in: can only test a child process: Exception ignored in: \n","<function _MultiProcessingDataLoaderIter.__del__ at 0x7f99a55cbee0>can only test a child process<function _MultiProcessingDataLoaderIter.__del__ at 0x7f99a55cbee0>\n","\n","\n","Exception ignored in: Traceback (most recent call last):\n","<function _MultiProcessingDataLoaderIter.__del__ at 0x7f99a55cbee0>Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f99a55cbee0>Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n","\n","\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","          File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n","self._shutdown_workers()self._shutdown_workers()\n","        self._shutdown_workers()\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n","self._shutdown_workers()\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n","    \n","  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n","    if w.is_alive():  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n","    if w.is_alive():\n","    \n","if w.is_alive():  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n","if w.is_alive():  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n","\n","    \n","  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n","    assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n","assert self._parent_pid == os.getpid(), 'can only test a child process'    \n","    \n","assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'\n","AssertionErrorAssertionErrorAssertionError\n",": : : can only test a child processAssertionErrorcan only test a child process\n",": can only test a child process\n","can only test a child process\n","\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-155-148b419d5412>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m        \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m            \u001b[0mpose_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pose'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1331\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1333\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1357\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1360\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-46-e5ec71269359>\", line 121, in __getitem__\n    c[key] = Image.open(osp.join(self.data_path, 'cloth', c_name[key])).convert('RGB')\n  File \"/usr/local/lib/python3.8/dist-packages/PIL/Image.py\", line 2843, in open\n    fp = builtins.open(filename, \"rb\")\nFileNotFoundError: [Errno 2] No such file or directory: '/content/test/cloth/05599_00.jpg'\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torchvision import models\n","import os\n","from torch.nn.utils import spectral_norm\n","import numpy as np\n","\n","import functools\n","\n","\n","class ConditionGenerator(nn.Module):\n","    def __init__(self, opt, input1_nc, input2_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d):\n","        super(ConditionGenerator, self).__init__()\n","        self.warp_feature = opt[\"warp_feature\"]\n","        self.out_layer_opt = opt[\"out_layer\"]\n","        \n","        self.ClothEncoder = nn.Sequential(\n","            ResBlock(input1_nc, ngf, norm_layer=norm_layer, scale='down'),  # 128\n","            ResBlock(ngf, ngf * 2, norm_layer=norm_layer, scale='down'),  # 64\n","            ResBlock(ngf * 2, ngf * 4, norm_layer=norm_layer, scale='down'),  # 32\n","            ResBlock(ngf * 4, ngf * 4, norm_layer=norm_layer, scale='down'),  # 16\n","            ResBlock(ngf * 4, ngf * 4, norm_layer=norm_layer, scale='down')  # 8\n","        )\n","        \n","        self.PoseEncoder = nn.Sequential(\n","            ResBlock(input2_nc, ngf, norm_layer=norm_layer, scale='down'),\n","            ResBlock(ngf, ngf * 2, norm_layer=norm_layer, scale='down'),\n","            ResBlock(ngf * 2, ngf * 4, norm_layer=norm_layer, scale='down'),\n","            ResBlock(ngf * 4, ngf * 4, norm_layer=norm_layer, scale='down'),\n","            ResBlock(ngf * 4, ngf * 4, norm_layer=norm_layer, scale='down')\n","        )\n","        \n","        self.conv = ResBlock(ngf * 4, ngf * 8, norm_layer=norm_layer, scale='same')\n","        \n","        if opt[\"warp_feature\"] == 'T1':\n","            # in_nc -> skip connection + T1, T2 channel\n","            self.SegDecoder = nn.Sequential(\n","                ResBlock(ngf * 8, ngf * 4, norm_layer=norm_layer, scale='up'),  # 16\n","                ResBlock(ngf * 4 * 2 + ngf * 4 , ngf * 4, norm_layer=norm_layer, scale='up'),  # 32\n","                ResBlock(ngf * 4 * 2 + ngf * 4 , ngf * 2, norm_layer=norm_layer, scale='up'),  # 64\n","                ResBlock(ngf * 2 * 2 + ngf * 4 , ngf, norm_layer=norm_layer, scale='up'),  # 128\n","                ResBlock(ngf * 1 * 2 + ngf * 4, ngf, norm_layer=norm_layer, scale='up')  # 256\n","            )\n","        if opt[\"warp_feature\"] == 'encoder':\n","            # in_nc -> [x, skip_connection, warped_cloth_encoder_feature(E1)]\n","            self.SegDecoder = nn.Sequential(\n","                ResBlock(ngf * 8, ngf * 4, norm_layer=norm_layer, scale='up'),  # 16\n","                ResBlock(ngf * 4 * 3, ngf * 4, norm_layer=norm_layer, scale='up'),  # 32\n","                ResBlock(ngf * 4 * 3, ngf * 2, norm_layer=norm_layer, scale='up'),  # 64\n","                ResBlock(ngf * 2 * 3, ngf, norm_layer=norm_layer, scale='up'),  # 128\n","                ResBlock(ngf * 1 * 3, ngf, norm_layer=norm_layer, scale='up')  # 256\n","            )\n","        if opt[\"out_layer\"] == 'relu':\n","            self.out_layer = ResBlock(ngf + input1_nc + input2_nc, output_nc, norm_layer=norm_layer, scale='same')\n","        if opt[\"out_layer\"] == 'conv':\n","            self.out_layer = nn.Sequential(\n","                ResBlock(ngf + input1_nc + input2_nc, ngf, norm_layer=norm_layer, scale='same'),\n","                nn.Conv2d(ngf, output_nc, kernel_size=1, bias=True)\n","            )\n","        \n","        # Cloth Conv 1x1\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(ngf, ngf * 4, kernel_size=1, bias=True),\n","            nn.Conv2d(ngf * 2, ngf * 4, kernel_size=1, bias=True),\n","            nn.Conv2d(ngf * 4, ngf * 4, kernel_size=1, bias=True),\n","            nn.Conv2d(ngf * 4, ngf * 4, kernel_size=1, bias=True),\n","        )\n","\n","        # Person Conv 1x1\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(ngf, ngf * 4, kernel_size=1, bias=True),\n","            nn.Conv2d(ngf * 2, ngf * 4, kernel_size=1, bias=True),\n","            nn.Conv2d(ngf * 4, ngf * 4, kernel_size=1, bias=True),\n","            nn.Conv2d(ngf * 4, ngf * 4, kernel_size=1, bias=True),\n","        )\n","        \n","        self.flow_conv = nn.ModuleList([\n","            nn.Conv2d(ngf * 8, 2, kernel_size=3, stride=1, padding=1, bias=True),\n","            nn.Conv2d(ngf * 8, 2, kernel_size=3, stride=1, padding=1, bias=True),\n","            nn.Conv2d(ngf * 8, 2, kernel_size=3, stride=1, padding=1, bias=True),\n","            nn.Conv2d(ngf * 8, 2, kernel_size=3, stride=1, padding=1, bias=True),\n","            nn.Conv2d(ngf * 8, 2, kernel_size=3, stride=1, padding=1, bias=True),\n","        ]\n","        )\n","        \n","        self.bottleneck = nn.Sequential(\n","            nn.Sequential(nn.Conv2d(ngf * 4, ngf * 4, kernel_size=3, stride=1, padding=1, bias=True), nn.ReLU()),\n","            nn.Sequential(nn.Conv2d(ngf * 4, ngf * 4, kernel_size=3, stride=1, padding=1, bias=True), nn.ReLU()),\n","            nn.Sequential(nn.Conv2d(ngf * 2, ngf * 4, kernel_size=3, stride=1, padding=1, bias=True) , nn.ReLU()),\n","            nn.Sequential(nn.Conv2d(ngf, ngf * 4, kernel_size=3, stride=1, padding=1, bias=True), nn.ReLU()),\n","        )\n","        \n","    def normalize(self, x):\n","        return x\n","    \n","    def forward(self,opt,input1, input2, upsample='bilinear'):\n","        E1_list = []\n","        E2_list = []\n","        flow_list = []\n","        # warped_grid_list = []\n","\n","        # Feature Pyramid Network\n","        for i in range(5):\n","            if i == 0:\n","                E1_list.append(self.ClothEncoder[i](input1))\n","                E2_list.append(self.PoseEncoder[i](input2))\n","            else:\n","                E1_list.append(self.ClothEncoder[i](E1_list[i - 1]))\n","                E2_list.append(self.PoseEncoder[i](E2_list[i - 1]))\n","\n","        # Compute Clothflow\n","        for i in range(5):\n","            N, _, iH, iW = E1_list[4 - i].size()\n","            grid = make_grid(N, iH, iW,opt)\n","\n","            if i == 0:\n","                T1 = E1_list[4 - i]  # (ngf * 4) x 8 x 6\n","                T2 = E2_list[4 - i]\n","                E4 = torch.cat([T1, T2], 1)\n","                \n","                flow = self.flow_conv[i](self.normalize(E4)).permute(0, 2, 3, 1)\n","                flow_list.append(flow)\n","                \n","                x = self.conv(T2)\n","                x = self.SegDecoder[i](x)\n","                \n","            else:\n","                T1 = F.interpolate(T1, scale_factor=2, mode=upsample) + self.conv1[4 - i](E1_list[4 - i])\n","                T2 = F.interpolate(T2, scale_factor=2, mode=upsample) + self.conv2[4 - i](E2_list[4 - i]) \n","                \n","                flow = F.interpolate(flow_list[i - 1].permute(0, 3, 1, 2), scale_factor=2, mode=upsample).permute(0, 2, 3, 1)  # upsample n-1 flow\n","                flow_norm = torch.cat([flow[:, :, :, 0:1] / ((iW/2 - 1.0) / 2.0), flow[:, :, :, 1:2] / ((iH/2 - 1.0) / 2.0)], 3)\n","                warped_T1 = F.grid_sample(T1, flow_norm + grid, padding_mode='border')\n","                \n","                flow = flow + self.flow_conv[i](self.normalize(torch.cat([warped_T1, self.bottleneck[i-1](x)], 1))).permute(0, 2, 3, 1)  # F(n)\n","                flow_list.append(flow)\n","\n","                if self.warp_feature == 'T1':\n","                    x = self.SegDecoder[i](torch.cat([x, E2_list[4-i], warped_T1], 1))\n","                if self.warp_feature == 'encoder':\n","                    warped_E1 = F.grid_sample(E1_list[4-i], flow_norm + grid, padding_mode='border')\n","                    x = self.SegDecoder[i](torch.cat([x, E2_list[4-i], warped_E1], 1))\n","        \n"," \n","        N, _, iH, iW = input1.size()\n","        grid = make_grid(N, iH, iW,opt)\n","        \n","        flow = F.interpolate(flow_list[-1].permute(0, 3, 1, 2), scale_factor=2, mode=upsample).permute(0, 2, 3, 1)\n","        flow_norm = torch.cat([flow[:, :, :, 0:1] / ((iW/2 - 1.0) / 2.0), flow[:, :, :, 1:2] / ((iH/2 - 1.0) / 2.0)], 3)\n","        warped_input1 = F.grid_sample(input1, flow_norm + grid, padding_mode='border')\n","        \n","        x = self.out_layer(torch.cat([x, input2, warped_input1], 1))\n","\n","        warped_c = warped_input1[:, :-1, :, :]\n","        warped_cm = warped_input1[:, -1:, :, :]\n","\n","        return flow_list, x, warped_c, warped_cm\n","\n","def make_grid(N, iH, iW,opt):\n","    grid_x = torch.linspace(-1.0, 1.0, iW).view(1, 1, iW, 1).expand(N, iH, -1, -1)\n","    grid_y = torch.linspace(-1.0, 1.0, iH).view(1, iH, 1, 1).expand(N, -1, iW, -1)\n","    if opt.cuda :\n","        grid = torch.cat([grid_x, grid_y], 3).cuda()\n","    else:\n","        grid = torch.cat([grid_x, grid_y], 3)\n","    return grid\n","\n","\n","class ResBlock(nn.Module):\n","    def __init__(self, in_nc, out_nc, scale='down', norm_layer=nn.BatchNorm2d):\n","        super(ResBlock, self).__init__()\n","        use_bias = norm_layer == nn.InstanceNorm2d\n","        assert scale in ['up', 'down', 'same'], \"ResBlock scale must be in 'up' 'down' 'same'\"\n","\n","        if scale == 'same':\n","            self.scale = nn.Conv2d(in_nc, out_nc, kernel_size=1, bias=True)\n","        if scale == 'up':\n","            self.scale = nn.Sequential(\n","                nn.Upsample(scale_factor=2, mode='bilinear'),\n","                nn.Conv2d(in_nc, out_nc, kernel_size=1,bias=True)\n","            )\n","        if scale == 'down':\n","            self.scale = nn.Conv2d(in_nc, out_nc, kernel_size=3, stride=2, padding=1, bias=use_bias)\n","            \n","        self.block = nn.Sequential(\n","            nn.Conv2d(out_nc, out_nc, kernel_size=3, stride=1, padding=1, bias=use_bias),\n","            norm_layer(out_nc),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_nc, out_nc, kernel_size=3, stride=1, padding=1, bias=use_bias),\n","            norm_layer(out_nc)\n","        )\n","        self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, x):\n","        residual = self.scale(x)\n","        return self.relu(residual + self.block(residual))\n","\n","\n","class Vgg19(nn.Module):\n","    def __init__(self, requires_grad=False):\n","        super(Vgg19, self).__init__()\n","        vgg_pretrained_features = models.vgg19(pretrained=True).features\n","        self.slice1 = torch.nn.Sequential()\n","        self.slice2 = torch.nn.Sequential()\n","        self.slice3 = torch.nn.Sequential()\n","        self.slice4 = torch.nn.Sequential()\n","        self.slice5 = torch.nn.Sequential()\n","        for x in range(2):\n","            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n","        for x in range(2, 7):\n","            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n","        for x in range(7, 12):\n","            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n","        for x in range(12, 21):\n","            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n","        for x in range(21, 30):\n","            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n","        if not requires_grad:\n","            for param in self.parameters():\n","                param.requires_grad = False\n","\n","    def forward(self, X):\n","        h_relu1 = self.slice1(X)\n","        h_relu2 = self.slice2(h_relu1)\n","        h_relu3 = self.slice3(h_relu2)\n","        h_relu4 = self.slice4(h_relu3)\n","        h_relu5 = self.slice5(h_relu4)\n","        out = [h_relu1, h_relu2, h_relu3, h_relu4, h_relu5]\n","        return out\n","    \n","\n","class VGGLoss(nn.Module):\n","    def __init__(self, opt,layids = None):\n","        super(VGGLoss, self).__init__()\n","        self.vgg = Vgg19()\n","        if opt.cuda:\n","            self.vgg.cuda()\n","        self.criterion = nn.L1Loss()\n","        self.weights = [1.0/32, 1.0/16, 1.0/8, 1.0/4, 1.0]\n","        self.layids = layids\n","\n","    def forward(self, x, y):\n","        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n","        loss = 0\n","        if self.layids is None:\n","            self.layids = list(range(len(x_vgg)))\n","        for i in self.layids:\n","            loss += self.weights[i] * self.criterion(x_vgg[i], y_vgg[i].detach())\n","        return loss\n","\n","# Defines the GAN loss which uses either LSGAN or the regular GAN.\n","# When LSGAN is used, it is basically same as MSELoss,\n","# but it abstracts away the need to create the target label tensor\n","# that has the same size as the input\n","\n","class GANLoss(nn.Module):\n","    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0,\n","                 tensor=torch.FloatTensor):\n","        super(GANLoss, self).__init__()\n","        self.real_label = target_real_label\n","        self.fake_label = target_fake_label\n","        self.real_label_var = None\n","        self.fake_label_var = None\n","        self.Tensor = tensor\n","        if use_lsgan:\n","            self.loss = nn.MSELoss()\n","        else:\n","            self.loss = nn.BCELoss()\n","\n","    def get_target_tensor(self, input, target_is_real):\n","        if target_is_real:\n","            create_label = ((self.real_label_var is None) or\n","                            (self.real_label_var.numel() != input.numel()))\n","            if create_label:\n","                real_tensor = self.Tensor(input.size()).fill_(self.real_label)\n","                self.real_label_var = Variable(real_tensor, requires_grad=False)\n","            target_tensor = self.real_label_var\n","        else:\n","            create_label = ((self.fake_label_var is None) or\n","                            (self.fake_label_var.numel() != input.numel()))\n","            if create_label:\n","                fake_tensor = self.Tensor(input.size()).fill_(self.fake_label)\n","                self.fake_label_var = Variable(fake_tensor, requires_grad=False)\n","            target_tensor = self.fake_label_var\n","        return target_tensor\n","\n","    def __call__(self, input, target_is_real):\n","        if isinstance(input[0], list):\n","            loss = 0\n","            for input_i in input:\n","                pred = input_i[-1]\n","                target_tensor = self.get_target_tensor(pred, target_is_real)\n","                loss += self.loss(pred, target_tensor)\n","            return loss\n","        else:\n","            target_tensor = self.get_target_tensor(input[-1], target_is_real)\n","            return self.loss(input[-1], target_tensor)\n","\n","\n","class MultiscaleDiscriminator(nn.Module):\n","    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d,\n","                 use_sigmoid=False, num_D=3, getIntermFeat=False, Ddownx2=False, Ddropout=False, spectral=False):\n","        super(MultiscaleDiscriminator, self).__init__()\n","        self.num_D = num_D\n","        self.n_layers = n_layers\n","        self.getIntermFeat = getIntermFeat\n","        self.Ddownx2 = Ddownx2\n","\n","\n","        for i in range(num_D):\n","            netD = NLayerDiscriminator(input_nc, ndf, n_layers, norm_layer, use_sigmoid, getIntermFeat, Ddropout, spectral=spectral)\n","            if getIntermFeat:\n","                for j in range(n_layers + 2):\n","                    setattr(self, 'scale' + str(i) + '_layer' + str(j), getattr(netD, 'model' + str(j)))\n","            else:\n","                setattr(self, 'layer' + str(i), netD.model)\n","\n","        self.downsample = nn.AvgPool2d(3, stride=2, padding=[1, 1], count_include_pad=False)\n","\n","    def singleD_forward(self, model, input):\n","        if self.getIntermFeat:\n","            result = [input]\n","            for i in range(len(model)):\n","                result.append(model[i](result[-1]))\n","            return result[1:]\n","        else:\n","            return [model(input)]\n","\n","    def forward(self, input):\n","        num_D = self.num_D\n","        \n","        result = []\n","        if self.Ddownx2:\n","            input_downsampled = self.downsample(input)\n","        else:\n","            input_downsampled = input\n","        for i in range(num_D):\n","            \n","            if self.getIntermFeat:\n","                model = [getattr(self, 'scale' + str(num_D - 1 - i) + '_layer' + str(j)) for j in\n","                         range(self.n_layers + 2)]\n","            else:\n","                model = getattr(self, 'layer' + str(num_D - 1 - i))\n","            result.append(self.singleD_forward(model, input_downsampled))\n","            if i != (num_D - 1):\n","                input_downsampled = self.downsample(input_downsampled)\n","        return result\n","\n","class NLayerDiscriminator(nn.Module):\n","    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False, getIntermFeat=False, Ddropout=False, spectral=False):\n","        super(NLayerDiscriminator, self).__init__()\n","        self.getIntermFeat = getIntermFeat\n","        self.n_layers = n_layers\n","        self.spectral_norm = spectral_norm if spectral else lambda x: x\n","\n","        kw = 4\n","        padw = int(np.ceil((kw - 1.0) / 2))\n","        sequence = [[nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]]\n","\n","        nf = ndf\n","        for n in range(1, n_layers):\n","            nf_prev = nf\n","            nf = min(nf * 2, 512)\n","            if Ddropout:\n","                sequence += [[\n","                self.spectral_norm(nn.Conv2d(nf_prev, nf, kernel_size=kw, stride=2, padding=padw)),\n","                norm_layer(nf), nn.LeakyReLU(0.2, True), nn.Dropout(0.5)\n","            ]]\n","            else:\n","\n","                sequence += [[\n","                    self.spectral_norm(nn.Conv2d(nf_prev, nf, kernel_size=kw, stride=2, padding=padw)),\n","                    norm_layer(nf), nn.LeakyReLU(0.2, True)\n","                ]]\n","                \n","        nf_prev = nf\n","        nf = min(nf * 2, 512)\n","        sequence += [[\n","            nn.Conv2d(nf_prev, nf, kernel_size=kw, stride=1, padding=padw),\n","            norm_layer(nf),\n","            nn.LeakyReLU(0.2, True)\n","        ]]\n","\n","        sequence += [[nn.Conv2d(nf, 1, kernel_size=kw, stride=1, padding=padw)]]\n","\n","        if use_sigmoid:\n","            sequence += [[nn.Sigmoid()]]\n","\n","        if getIntermFeat:\n","            for n in range(len(sequence)):\n","                setattr(self, 'model' + str(n), nn.Sequential(*sequence[n]))\n","        else:\n","            sequence_stream = []\n","            for n in range(len(sequence)):\n","                sequence_stream += sequence[n]\n","            self.model = nn.Sequential(*sequence_stream)\n","\n","    def forward(self, input):\n","        if self.getIntermFeat:\n","            res = [input]\n","            for n in range(self.n_layers + 2):\n","                model = getattr(self, 'model' + str(n))\n","                res.append(model(res[-1]))\n","            return res[1:]\n","        else:\n","            return self.model(input)\n","\n","\n","def save_checkpoint(model, save_path,opt):\n","    if not os.path.exists(os.path.dirname(save_path)):\n","        os.makedirs(os.path.dirname(save_path))\n","\n","    torch.save(model.cpu().state_dict(), save_path)\n","    if opt.cuda :\n","        model.cuda()\n","\n","def load_checkpoint(model, checkpoint_path,opt):\n","    if not os.path.exists(checkpoint_path):\n","        print('no checkpoint')\n","        raise\n","    log = model.load_state_dict(torch.load(checkpoint_path), strict=False)\n","    if opt.cuda :\n","        model.cuda()\n","\n","\n","def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv2d') != -1:\n","        m.weight.data.normal_(0.0, 0.02)\n","    elif classname.find('BatchNorm2d') != -1:\n","        m.weight.data.normal_(1.0, 0.02)\n","        m.bias.data.fill_(0)\n","\n","def get_norm_layer(norm_type='instance'):\n","    if norm_type == 'batch':\n","        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n","    elif norm_type == 'instance':\n","        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False)\n","    else:\n","        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n","    return norm_layer\n","\n","def define_D(input_nc, ndf=64, n_layers_D=3, norm='instance', use_sigmoid=False, num_D=2, getIntermFeat=False, gpu_ids=[], Ddownx2=False, Ddropout=False, spectral=False):\n","    norm_layer = get_norm_layer(norm_type=norm)\n","    netD = MultiscaleDiscriminator(input_nc, ndf, n_layers_D, norm_layer, use_sigmoid, num_D, getIntermFeat, Ddownx2, Ddropout, spectral=spectral)\n","    print(netD)\n","    if len(gpu_ids) > 0:\n","        assert (torch.cuda.is_available())\n","        netD.cuda()\n","    netD.apply(weights_init)\n","    return netD\n"],"metadata":{"id":"kSSJegaW9Syn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.utils.data as data\n","import torchvision.transforms as transforms\n","\n","from PIL import Image, ImageDraw\n","\n","import os.path as osp\n","import numpy as np\n","import json\n","\n","\n","class CPDatasetTest(data.Dataset):\n","    \"\"\"\n","        Test Dataset for CP-VTON.\n","    \"\"\"\n","    def __init__(self, opt):\n","        super(CPDatasetTest, self).__init__()\n","        # base setting\n","        self.opt = opt\n","        self.root = opt[\"root\"]\n","        self.datamode = opt[\"datamode\"] # train or test or self-defined\n","        self.data_list = opt[\"data_list\"]\n","        self.fine_height = opt[\"fine_height\"]\n","        self.fine_width = opt[\"fine_width\"]\n","        self.semantic_nc = opt[\"semantic\"]\n","        self.data_path = opt[\"datapath\"]\n","        self.transform = transforms.Compose([  \\\n","                transforms.ToTensor(),   \\\n","                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","        # load data list\n","        im_names = []\n","        c_names = []\n","        with open(\"train_pairs.txt\", 'r') as f:\n","            for line in f.readlines():\n","                im_name, c_name = line.strip().split()\n","                im_names.append(im_name)\n","                c_names.append(c_name)\n","\n","        self.im_names = im_names\n","        self.c_names = dict()\n","        self.c_names['paired'] = im_names\n","        self.c_names['unpaired'] = c_names\n","\n","    def name(self):\n","        return \"CPDataset\"\n","    def get_agnostic(self, im, im_parse, pose_data):\n","        parse_array = np.array(im_parse)\n","        parse_head = ((parse_array == 4).astype(np.float32) +\n","                      (parse_array == 13).astype(np.float32))\n","        parse_lower = ((parse_array == 9).astype(np.float32) +\n","                       (parse_array == 12).astype(np.float32) +\n","                       (parse_array == 16).astype(np.float32) +\n","                       (parse_array == 17).astype(np.float32) +\n","                       (parse_array == 18).astype(np.float32) +\n","                       (parse_array == 19).astype(np.float32))\n","\n","        agnostic = im.copy()\n","        agnostic_draw = ImageDraw.Draw(agnostic)\n","\n","        length_a = np.linalg.norm(pose_data[5] - pose_data[2])\n","        length_b = np.linalg.norm(pose_data[12] - pose_data[9])\n","        point = (pose_data[9] + pose_data[12]) / 2\n","        pose_data[9] = point + (pose_data[9] - point) / length_b * length_a\n","        pose_data[12] = point + (pose_data[12] - point) / length_b * length_a\n","\n","        r = int(length_a / 16) + 1\n","\n","        # mask torso\n","        for i in [9, 12]:\n","            pointx, pointy = pose_data[i]\n","            agnostic_draw.ellipse((pointx-r*3, pointy-r*6, pointx+r*3, pointy+r*6), 'gray', 'gray')\n","        agnostic_draw.line([tuple(pose_data[i]) for i in [2, 9]], 'gray', width=r*6)\n","        agnostic_draw.line([tuple(pose_data[i]) for i in [5, 12]], 'gray', width=r*6)\n","        agnostic_draw.line([tuple(pose_data[i]) for i in [9, 12]], 'gray', width=r*12)\n","        agnostic_draw.polygon([tuple(pose_data[i]) for i in [2, 5, 12, 9]], 'gray', 'gray')\n","\n","        # mask neck\n","        pointx, pointy = pose_data[1]\n","        agnostic_draw.rectangle((pointx-r*5, pointy-r*9, pointx+r*5, pointy), 'gray', 'gray')\n","\n","        # mask arms\n","        agnostic_draw.line([tuple(pose_data[i]) for i in [2, 5]], 'gray', width=r*12)\n","        for i in [2, 5]:\n","            pointx, pointy = pose_data[i]\n","            agnostic_draw.ellipse((pointx-r*5, pointy-r*6, pointx+r*5, pointy+r*6), 'gray', 'gray')\n","        for i in [3, 4, 6, 7]:\n","            if (pose_data[i-1, 0] == 0.0 and pose_data[i-1, 1] == 0.0) or (pose_data[i, 0] == 0.0 and pose_data[i, 1] == 0.0):\n","                continue\n","            agnostic_draw.line([tuple(pose_data[j]) for j in [i - 1, i]], 'gray', width=r*10)\n","            pointx, pointy = pose_data[i]\n","            agnostic_draw.ellipse((pointx-r*5, pointy-r*5, pointx+r*5, pointy+r*5), 'gray', 'gray')\n","\n","        for parse_id, pose_ids in [(14, [5, 6, 7]), (15, [2, 3, 4])]:\n","            mask_arm = Image.new('L', (768, 1024), 'white')\n","            mask_arm_draw = ImageDraw.Draw(mask_arm)\n","            pointx, pointy = pose_data[pose_ids[0]]\n","            mask_arm_draw.ellipse((pointx-r*5, pointy-r*6, pointx+r*5, pointy+r*6), 'black', 'black')\n","            for i in pose_ids[1:]:\n","                if (pose_data[i-1, 0] == 0.0 and pose_data[i-1, 1] == 0.0) or (pose_data[i, 0] == 0.0 and pose_data[i, 1] == 0.0):\n","                    continue\n","                mask_arm_draw.line([tuple(pose_data[j]) for j in [i - 1, i]], 'black', width=r*10)\n","                pointx, pointy = pose_data[i]\n","                if i != pose_ids[-1]:\n","                    mask_arm_draw.ellipse((pointx-r*5, pointy-r*5, pointx+r*5, pointy+r*5), 'black', 'black')\n","            mask_arm_draw.ellipse((pointx-r*4, pointy-r*4, pointx+r*4, pointy+r*4), 'black', 'black')\n","\n","            parse_arm = (np.array(mask_arm) / 255) * (parse_array == parse_id).astype(np.float32)\n","            agnostic.paste(im, None, Image.fromarray(np.uint8(parse_arm * 255), 'L'))\n","\n","        agnostic.paste(im, None, Image.fromarray(np.uint8(parse_head * 255), 'L'))\n","        agnostic.paste(im, None, Image.fromarray(np.uint8(parse_lower * 255), 'L'))\n","        return agnostic\n","    def __getitem__(self, index):\n","        im_name = self.im_names[index]\n","        c_name = {}\n","        c = {}\n","        cm = {}\n","        for key in self.c_names:\n","            c_name[key] = self.c_names[key][index]\n","            c[key] = Image.open(osp.join(self.data_path, 'cloth', c_name[key])).convert('RGB')\n","            c[key] = transforms.Resize(self.fine_width, interpolation=2)(c[key])\n","            cm[key] = Image.open(osp.join(self.data_path, 'cloth-mask', c_name[key]))\n","            cm[key] = transforms.Resize(self.fine_width, interpolation=0)(cm[key])\n","\n","            c[key] = self.transform(c[key])  # [-1,1]\n","            cm_array = np.array(cm[key])\n","            cm_array = (cm_array >= 128).astype(np.float32)\n","            cm[key] = torch.from_numpy(cm_array)  # [0,1]\n","            cm[key].unsqueeze_(0)\n","\n","        # person image\n","        im_pil_big = Image.open(osp.join(self.data_path, 'image', im_name))\n","        im_pil = transforms.Resize(self.fine_width, interpolation=2)(im_pil_big)\n","        \n","        im = self.transform(im_pil)\n","\n","        # load parsing image\n","        parse_name = im_name.replace('.jpg', '.png')\n","        im_parse_pil_big = Image.open(osp.join(self.data_path, 'image-parse-v3', parse_name))\n","        im_parse_pil = transforms.Resize(self.fine_width, interpolation=0)(im_parse_pil_big)\n","        parse = torch.from_numpy(np.array(im_parse_pil)[None]).long()\n","        im_parse = self.transform(im_parse_pil.convert('RGB'))\n","        \n","        labels = {\n","            0:  ['background',  [0, 10]],\n","            1:  ['hair',        [1, 2]],\n","            2:  ['face',        [4, 13]],\n","            3:  ['upper',       [5, 6, 7]],\n","            4:  ['bottom',      [9, 12]],\n","            5:  ['left_arm',    [14]],\n","            6:  ['right_arm',   [15]],\n","            7:  ['left_leg',    [16]],\n","            8:  ['right_leg',   [17]],\n","            9:  ['left_shoe',   [18]],\n","            10: ['right_shoe',  [19]],\n","            11: ['socks',       [8]],\n","            12: ['noise',       [3, 11]]\n","        }\n","\n","        parse_map = torch.FloatTensor(20, self.fine_height, self.fine_width).zero_()\n","        parse_map = parse_map.scatter_(0, parse, 1.0)\n","        new_parse_map = torch.FloatTensor(self.semantic_nc, self.fine_height, self.fine_width).zero_()\n","        \n","        for i in range(len(labels)):\n","            for label in labels[i][1]:\n","                new_parse_map[i] += parse_map[label]\n","        \n","        parse_onehot = torch.FloatTensor(1, self.fine_height, self.fine_width).zero_()\n","        for i in range(len(labels)):\n","            for label in labels[i][1]:\n","                parse_onehot[0] += parse_map[label] * i\n","\n","        # load image-parse-agnostic\n","        image_parse_agnostic = Image.open(osp.join(self.data_path, 'image-parse-agnostic-v3.2', parse_name))\n","        image_parse_agnostic = transforms.Resize(self.fine_width, interpolation=0)(image_parse_agnostic)\n","        parse_agnostic = torch.from_numpy(np.array(image_parse_agnostic)[None]).long()\n","        image_parse_agnostic = self.transform(image_parse_agnostic.convert('RGB'))\n","\n","        parse_agnostic_map = torch.FloatTensor(20, self.fine_height, self.fine_width).zero_()\n","        parse_agnostic_map = parse_agnostic_map.scatter_(0, parse_agnostic, 1.0)\n","        new_parse_agnostic_map = torch.FloatTensor(self.semantic_nc, self.fine_height, self.fine_width).zero_()\n","        for i in range(len(labels)):\n","            for label in labels[i][1]:\n","                new_parse_agnostic_map[i] += parse_agnostic_map[label]\n","                \n","\n","        # parse cloth & parse cloth mask\n","        pcm = new_parse_map[3:4]\n","        im_c = im * pcm + (1 - pcm)\n","        \n","        # load pose points\n","        pose_name = im_name.replace('.jpg', '_rendered.png')\n","        pose_map = Image.open(osp.join(self.data_path, 'openpose_img', pose_name))\n","        pose_map = transforms.Resize(self.fine_width, interpolation=2)(pose_map)\n","        pose_map = self.transform(pose_map)  # [-1,1]\n","        \n","        pose_name = im_name.replace('.jpg', '_keypoints.json')\n","        with open(osp.join(self.data_path, 'openpose_json', pose_name), 'r') as f:\n","            pose_label = json.load(f)\n","            pose_data = pose_label['people'][0]['pose_keypoints_2d']\n","            pose_data = np.array(pose_data)\n","            pose_data = pose_data.reshape((-1, 3))[:, :2]\n","\n","        \n","        # load densepose\n","        densepose_name = im_name.replace('image', 'image-densepose')\n","        densepose_map = Image.open(osp.join(self.data_path, 'image-densepose', densepose_name))\n","        densepose_map = transforms.Resize(self.fine_width, interpolation=2)(densepose_map)\n","        densepose_map = self.transform(densepose_map)  # [-1,1]\n","        agnostic = self.get_agnostic(im_pil_big, im_parse_pil_big, pose_data)\n","        agnostic = transforms.Resize(self.fine_width, interpolation=2)(agnostic)\n","        agnostic = self.transform(agnostic)\n","        \n","\n","\n","        result = {\n","            'c_name':   c_name,     # for visualization\n","            'im_name':  im_name,    # for visualization or ground truth\n","            # intput 1 (clothfloww)\n","            'cloth':    c,          # for input\n","            'cloth_mask':     cm,   # for input\n","            # intput 2 (segnet)\n","            'parse_agnostic': new_parse_agnostic_map,\n","            'densepose': densepose_map,\n","            'pose': pose_map,       # for conditioning\n","            # GT\n","            'parse_onehot' : parse_onehot,  # Cross Entropy\n","            'parse': new_parse_map, # GAN Loss real\n","            'pcm': pcm,             # L1 Loss & vis\n","            'parse_cloth': im_c,    # VGG Loss & vis\n","            # visualization\n","            'image':    im,         # for visualization\n","            'agnostic' : agnostic\n","            }\n","        \n","        return result\n","\n","    def __len__(self):\n","        return len(self.im_names)\n","    \n","\n","class CPDataLoader(object):\n","    def __init__(self, opt, dataset):\n","        super(CPDataLoader, self).__init__()\n","        if opt[\"shuffle\"] :\n","            train_sampler = torch.utils.data.sampler.RandomSampler(dataset)\n","        else:\n","            train_sampler = None\n","\n","        self.data_loader = torch.utils.data.DataLoader(\n","                dataset, batch_size=opt[\"batch_size\"], shuffle=(train_sampler is None),\n","                num_workers=opt[\"workers\"], pin_memory=True, drop_last=True, sampler=train_sampler)\n","        self.dataset = dataset\n","        self.data_iter = self.data_loader.__iter__()\n","\n","    def next_batch(self):\n","        try:\n","            batch = self.data_iter.__next__()\n","        except StopIteration:\n","            self.data_iter = self.data_loader.__iter__()\n","            batch = self.data_iter.__next__()\n","\n","        return batch"],"metadata":{"id":"zG6gvdSq1VG6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_opt():\n","    parser = argparse.ArgumentParser()\n","\n","    parser.add_argument(\"--gpu_ids\", default=\"\")\n","    parser.add_argument('-j', '--workers', type=int, default=4)\n","    parser.add_argument('-b', '--batch-size', type=int, default=1)\n","    parser.add_argument('--fp16', action='store_true', help='use amp')\n","    # Cuda availability\n","    parser.add_argument('--cuda',default=False, help='cuda or cpu')\n","\n","    parser.add_argument('--test_name', type=str, default='test', help='test name')\n","    parser.add_argument(\"--dataroot\", default=\"./data/zalando-hd-resize\")\n","    parser.add_argument(\"--datamode\", default=\"test\")\n","    parser.add_argument(\"--data_list\", default=\"test_pairs.txt\")\n","    parser.add_argument(\"--output_dir\", type=str, default=\"./Output\")\n","    parser.add_argument(\"--datasetting\", default=\"unpaired\")\n","    parser.add_argument(\"--fine_width\", type=int, default=768)\n","    parser.add_argument(\"--fine_height\", type=int, default=1024)\n","\n","    parser.add_argument('--tensorboard_dir', type=str, default='./data/zalando-hd-resize/tensorboard', help='save tensorboard infos')\n","    parser.add_argument('--checkpoint_dir', type=str, default='checkpoints', help='save checkpoint infos')\n","    parser.add_argument('--tocg_checkpoint', type=str, default='./eval_models/weights/v0.1/mtviton.pth', help='tocg checkpoint')\n","    parser.add_argument('--gen_checkpoint', type=str, default='./eval_models/weights/v0.1/gen.pth', help='G checkpoint')\n","\n","    parser.add_argument(\"--tensorboard_count\", type=int, default=100)\n","    parser.add_argument(\"--shuffle\", action='store_true', help='shuffle input data')\n","    parser.add_argument(\"--semantic_nc\", type=int, default=13)\n","    parser.add_argument(\"--output_nc\", type=int, default=13)\n","    parser.add_argument('--gen_semantic_nc', type=int, default=7, help='# of input label classes without unknown class')\n","    \n","    # network\n","    parser.add_argument(\"--warp_feature\", choices=['encoder', 'T1'], default=\"T1\")\n","    parser.add_argument(\"--out_layer\", choices=['relu', 'conv'], default=\"relu\")\n","    \n","    # training\n","    parser.add_argument(\"--clothmask_composition\", type=str, choices=['no_composition', 'detach', 'warp_grad'], default='warp_grad')\n","        \n","    # Hyper-parameters\n","    parser.add_argument('--upsample', type=str, default='bilinear', choices=['nearest', 'bilinear'])\n","    parser.add_argument('--occlusion', action='store_true', help=\"Occlusion handling\")\n","\n","    # generator\n","    parser.add_argument('--norm_G', type=str, default='spectralaliasinstance', help='instance normalization or batch normalization')\n","    parser.add_argument('--ngf', type=int, default=64, help='# of gen filters in first conv layer')\n","    parser.add_argument('--init_type', type=str, default='xavier', help='network initialization [normal|xavier|kaiming|orthogonal]')\n","    parser.add_argument('--init_variance', type=float, default=0.02, help='variance of the initialization distribution')\n","    parser.add_argument('--num_upsampling_layers', choices=('normal', 'more', 'most'), default='most', # normal: 256, more: 512\n","                        help=\"If 'more', adds upsampling layer between the two middle resnet blocks. If 'most', also add one more upsampling + resnet layer at the end of the generator\")\n","\n","    opt = parser.parse_args()\n","    return opt\n"],"metadata":{"id":"kOfKitK_tvO1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["opt = get_opt()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":602},"id":"8ZOVujuNtzvl","executionInfo":{"status":"error","timestamp":1675334477939,"user_tz":-180,"elapsed":975,"user":{"displayName":"Mahmoud Atia Ead Atia","userId":"11781797524477583668"}},"outputId":"a725e729-cf50-4957-f45f-5e0787e5e06b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["usage: ipykernel_launcher.py [-h] [--gpu_ids GPU_IDS] [-j WORKERS]\n","                             [-b BATCH_SIZE] [--fp16] [--cuda CUDA]\n","                             [--test_name TEST_NAME] [--dataroot DATAROOT]\n","                             [--datamode DATAMODE] [--data_list DATA_LIST]\n","                             [--output_dir OUTPUT_DIR]\n","                             [--datasetting DATASETTING]\n","                             [--fine_width FINE_WIDTH]\n","                             [--fine_height FINE_HEIGHT]\n","                             [--tensorboard_dir TENSORBOARD_DIR]\n","                             [--checkpoint_dir CHECKPOINT_DIR]\n","                             [--tocg_checkpoint TOCG_CHECKPOINT]\n","                             [--gen_checkpoint GEN_CHECKPOINT]\n","                             [--tensorboard_count TENSORBOARD_COUNT]\n","                             [--shuffle] [--semantic_nc SEMANTIC_NC]\n","                             [--output_nc OUTPUT_NC]\n","                             [--gen_semantic_nc GEN_SEMANTIC_NC]\n","                             [--warp_feature {encoder,T1}]\n","                             [--out_layer {relu,conv}]\n","                             [--clothmask_composition {no_composition,detach,warp_grad}]\n","                             [--upsample {nearest,bilinear}] [--occlusion]\n","                             [--norm_G NORM_G] [--ngf NGF]\n","                             [--init_type INIT_TYPE]\n","                             [--init_variance INIT_VARIANCE]\n","                             [--num_upsampling_layers {normal,more,most}]\n","ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-67cdf687-8232-43f5-804a-b6f3a917111c.json\n"]},{"output_type":"error","ename":"SystemExit","evalue":"ignored","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ncwfphPVk1bp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" pose_map = inputs['pose']\n","                pre_clothes_mask = inputs['cloth_mask'][opt.datasetting]\n","                label = inputs['parse']\n","                parse_agnostic = inputs['parse_agnostic']\n","                agnostic = inputs['agnostic']\n","                clothes = inputs['cloth'][opt.datasetting] # target cloth\n","                densepose = inputs['densepose']\n","                im = inputs['image']\n","                input_label, input_parse_agnostic = label, parse_agnostic\n","                pre_clothes_mask = torch.FloatTensor((pre_clothes_mask.detach().cpu().numpy() > 0.5).astype(np.float))\n"],"metadata":{"id":"Ftx6S5Xdgnta"},"execution_count":null,"outputs":[]}]}